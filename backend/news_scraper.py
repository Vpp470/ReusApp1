"""
Sistema de scraping i processament de not√≠cies locals de Reus
Utilitza RSS feeds i scraping millorat per m√†xima fiabilitat
"""
import asyncio
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Dict
import os
from dotenv import load_dotenv
# from emergentintegrations.llm.chat import LlmChat, UserMessage
from openai import OpenAI
import feedparser

load_dotenv()

# RSS Feeds (m√©s fiables que scraping)
RSS_FEEDS = {
    "diari_mes_reus": {
        "url": "https://www.diarimes.com/ca/rss/reus.xml",
        "name": "Diari M√©s - Reus",
        "rss": True
    },
    "diari_mes_camp": {
        "url": "https://www.diarimes.com/ca/rss/camp-tarragona.xml",
        "name": "Diari M√©s - Camp de Tarragona",
        "rss": True
    },
    "canal_reus": {
        "url": "https://canalreus.cat/feed/",
        "name": "Canal Reus",
        "rss": True  # S√ç t√© RSS feed!
    },
    "reusdigital": {
        "url": "https://www.reusdigital.cat/rss",
        "name": "Reus Digital",
        "rss": True
    }
}

# URL Agenda Municipal
AGENDA_MUNICIPAL_URL = "https://www.reus.cat/ajuntament/lajuntament-informa/agenda"


async def fetch_from_rss(feed_url: str, source_name: str) -> List[Dict]:
    """
    Obtenir not√≠cies des d'un RSS feed
    """
    try:
        print(f"   üì° RSS {source_name}...")
        feed = feedparser.parse(feed_url)
        
        news_items = []
        for entry in feed.entries[:10]:  # M√†xim 10 per font
            title = entry.get('title', '').strip()
            link = entry.get('link', '').strip()
            
            # Canal Reus sempre √©s de Reus, no cal filtrar pel t√≠tol
            if source_name == "Canal Reus":
                if title and link:
                    news_items.append({
                        'title': title,
                        'url': link,
                        'source': source_name
                    })
            # Per altres fonts, filtrar per "reus" al t√≠tol
            elif title and link and 'reus' in title.lower():
                news_items.append({
                    'title': title,
                    'url': link,
                    'source': source_name
                })
        
        print(f"      ‚úÖ {len(news_items)} not√≠cies trobades")
        return news_items
    
    except Exception as e:
        print(f"      ‚ùå Error RSS {source_name}: {str(e)}")
        return []


async def scrape_news_from_url(url: str, source_name: str) -> List[Dict]:
    """
    Scraping millorat de not√≠cies d'una URL espec√≠fica
    Busca en m√∫ltiples llocs i estructures HTML
    """
    try:
        print(f"   üîç Scraping {source_name}...")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        
        if response.status_code != 200:
            print(f"      ‚ùå Error {response.status_code}")
            return []
        
        soup = BeautifulSoup(response.content, 'html.parser')
        news_items = []
        
        # Estrat√®gia 1: Buscar articles amb classes comunes
        selectors = [
            ('article', None),
            ('div', ['noticia', 'article', 'news-item', 'entry', 'post', 'item']),
            ('li', ['news-list-item', 'article-item']),
        ]
        
        for tag, classes in selectors:
            if classes:
                for cls in classes:
                    articles = soup.find_all(tag, class_=lambda x: x and cls in x if x else False)
                    if articles:
                        break
            else:
                articles = soup.find_all(tag)
            
            if articles and len(articles) > 2:
                break
        
        # Estrat√®gia 2: Si no troba articles, buscar tots els enlla√ßos amb t√≠tols
        if not articles or len(articles) < 3:
            articles = soup.find_all('a', href=True)
        
        for article in articles[:15]:  # Limitar a 15 per font
            try:
                # Buscar t√≠tol
                title_elem = article.find(['h1', 'h2', 'h3', 'h4', 'span', 'strong'])
                if not title_elem:
                    title_elem = article
                
                title = title_elem.get_text(strip=True)
                
                # Buscar enlla√ß
                if article.name == 'a':
                    link = article.get('href', '')
                else:
                    link_elem = article.find('a', href=True)
                    link = link_elem['href'] if link_elem else ''
                
                # Filtrar not√≠cies v√†lides
                if title and link and len(title) > 20 and len(title) < 200:
                    # Assegurar URL absoluta
                    if not link.startswith('http'):
                        from urllib.parse import urljoin
                        link = urljoin(url, link)
                    
                    # Filtrar per "reus" al t√≠tol o URL
                    if 'reus' in title.lower() or 'reus' in link.lower():
                        news_items.append({
                            'title': title,
                            'url': link,
                            'source': source_name
                        })
            except:
                continue
        
        # Eliminar duplicats per URL
        seen_urls = set()
        unique_news = []
        for item in news_items:
            if item['url'] not in seen_urls:
                seen_urls.add(item['url'])
                unique_news.append(item)
        
        print(f"      ‚úÖ {len(unique_news)} not√≠cies trobades")
        return unique_news[:10]  # M√†xim 10
    
    except Exception as e:
        print(f"      ‚ùå Error: {str(e)}")
        return []


async def process_news_with_ai(raw_news: List[Dict], max_news: int = 6) -> List[Dict]:
    """
    Processar not√≠cies amb IA per filtrar i resumir les m√©s rellevants
    Si no hi ha clau d'API o falla, retorna les primeres not√≠cies
    """
    try:
        api_key = os.getenv('EMERGENT_LLM_KEY') or os.getenv('OPENAI_API_KEY')
        
        # Si no hi ha clau, retornar not√≠cies sense processar
        if not api_key:
            print("   ‚ö†Ô∏è Sense clau d'IA - usant selecci√≥ autom√†tica sense IA")
            # Prioritzar not√≠cies de Canal Reus i Reus Digital (m√©s locals)
            priority_sources = ["Canal Reus", "Reus Digital"]
            priority_news = [n for n in raw_news if n.get('source') in priority_sources]
            other_news = [n for n in raw_news if n.get('source') not in priority_sources]
            return (priority_news + other_news)[:max_news]
        
        # Preparar prompt per a la IA
        news_text = "\n\n".join([
            f"{i+1}. {item['title']} (Font: {item['source']})\n   URL: {item['url']}"
            for i, item in enumerate(raw_news)
        ])
        
        prompt = f"""Ets un editor de not√≠cies local de Reus. Tens aquesta llista de not√≠cies:

{news_text}

TASCA:
1. Selecciona les {max_news} not√≠cies M√âS RELLEVANTS sobre Reus, el seu comer√ß local, esdeveniments o cultura
2. Descarta not√≠cies no relacionades amb Reus o poc interessants
3. Retorna NOM√âS els n√∫meros de les not√≠cies seleccionades separats per comes (exemple: 1,3,5,7)

RESPOSTA (NOM√âS N√öMEROS):"""

        # Cridar la IA amb OpenAI directament
        client = OpenAI(api_key=api_key)
        
        completion = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Ets un editor de not√≠cies local expert en seleccionar contingut rellevant."},
                {"role": "user", "content": prompt}
            ]
        )
        
        response = completion.choices[0].message.content
        
        # Extreure n√∫meros seleccionats
        selected_indices = [int(n.strip())-1 for n in response.split(',') if n.strip().isdigit()]
        
        # Filtrar not√≠cies seleccionades
        selected_news = [raw_news[i] for i in selected_indices if i < len(raw_news)]
        
        return selected_news[:max_news]
    
    except Exception as e:
        print(f"‚ùå Error processant amb IA: {str(e)}")
        # Fallback: retornar les primeres not√≠cies
        return raw_news[:max_news]


async def fetch_daily_news(max_news: int = 6) -> List[Dict]:
    """
    Obtenir not√≠cies di√†ries de totes les fonts
    Utilitza RSS primer, despr√©s scraping com a fallback
    """
    print(f"\nüîç Cercant not√≠cies de Reus... ({datetime.now().strftime('%H:%M')})")
    
    all_news = []
    
    # 1. Intentar RSS feeds primer (m√©s fiable)
    print(f"   üì° Provant RSS feeds...")
    for key, feed_info in RSS_FEEDS.items():
        if feed_info.get('rss'):
            news = await fetch_from_rss(feed_info['url'], feed_info['name'])
            all_news.extend(news)
            await asyncio.sleep(1)
    
    # 2. Si RSS no ha donat resultats, provar scraping
    if len(all_news) < 3:
        print(f"   üîç Provant scraping directe...")
        for key, feed_info in RSS_FEEDS.items():
            if not feed_info.get('rss'):  # Nom√©s scraping per fonts sense RSS
                news = await scrape_news_from_url(feed_info['url'], feed_info['name'])
                all_news.extend(news)
                await asyncio.sleep(1)
    
    print(f"   ‚úÖ Total not√≠cies trobades: {len(all_news)}")
    
    # Invertir l'ordre per tenir les m√©s recents primer
    all_news = list(reversed(all_news))
    
    # 3. Processar amb IA per seleccionar les m√©s rellevants
    if all_news:
        print(f"   ü§ñ Processant amb IA...")
        try:
            selected_news = await process_news_with_ai(all_news, max_news)
            print(f"   ‚úÖ Seleccionades: {len(selected_news)} not√≠cies\n")
            return selected_news
        except Exception as e:
            print(f"   ‚ö†Ô∏è  IA no disponible, retornant not√≠cies sense filtrar")
            # Retornar les m√©s recents quan la IA falla
            return all_news[:max_news]
    
    return []


# Test del scraper
if __name__ == "__main__":
    async def test():
        news = await fetch_daily_news(6)
        for i, item in enumerate(news, 1):
            print(f"{i}. {item['title']}")
            print(f"   Font: {item['source']}")
            print(f"   URL: {item['url']}\n")
    
    asyncio.run(test())
